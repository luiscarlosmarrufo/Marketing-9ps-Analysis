{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'tweet': [\n",
    "        \"I fucking hate starbucks coffee, so expensive\",\n",
    "        \"There is no starbucks near my city\",\n",
    "        \"I love starbucks caramel malchiato\",\n",
    "        \"I dont like the new printed notes on the coffee...\"\n",
    "    ],\n",
    "    'PRODUCT': [0, 0, 0, 0],\n",
    "    'PLACE': [0, 0, 0, 0],\n",
    "    'PRICE': [0, 0, 0, 0]\n",
    "})\n",
    "\n",
    "def classify_batch(tweets, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process multiple tweets in a single API call by constructing a batch prompt\n",
    "    \"\"\"\n",
    "    # Instantiate the Ollama client\n",
    "    client = ollama.Client()\n",
    "    \n",
    "    # Construct a batch prompt that asks for classification of multiple tweets\n",
    "    batch_prompt = \"\"\"\n",
    "You are an expert NLP classifier. Classify each tweet below into categories: PRODUCT, PLACE, and PRICE.\n",
    "For each tweet, determine if it belongs to each category (1) or not (0).\n",
    "Return your analysis in a valid JSON array where each item corresponds to one tweet in the same order:\n",
    "[\n",
    "  {\"PRODUCT\": 0 or 1, \"PLACE\": 0 or 1, \"PRICE\": 0 or 1},\n",
    "  {\"PRODUCT\": 0 or 1, \"PLACE\": 0 or 1, \"PRICE\": 0 or 1},\n",
    "  ...\n",
    "]\n",
    "\n",
    "Tweets to classify:\n",
    "\"\"\"\n",
    "    \n",
    "    # Add each tweet with an index\n",
    "    for i, tweet in enumerate(tweets):\n",
    "        batch_prompt += f\"\\n{i+1}. \\\"{tweet}\\\"\\n\"\n",
    "    \n",
    "    batch_prompt += \"\\nOnly output the JSON array with no additional text.\"\n",
    "    \n",
    "    # Make the API call\n",
    "    try:\n",
    "        result_str = client.chat(\n",
    "            model=\"llama3\",\n",
    "            messages=[{\"role\": \"user\", \"content\": batch_prompt}],\n",
    "            options={\"temperature\": 0}\n",
    "        )\n",
    "        \n",
    "        # Extract the content from the response\n",
    "        response_content = result_str['message']['content'].strip()\n",
    "        \n",
    "        # Find JSON in the response (in case model adds extra text)\n",
    "        import re\n",
    "        json_match = re.search(r'\\[.*\\]', response_content, re.DOTALL)\n",
    "        if json_match:\n",
    "            response_content = json_match.group(0)\n",
    "        \n",
    "        # Parse the JSON array\n",
    "        results = json.loads(response_content)\n",
    "        return results\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        print(f\"Response content: {response_content if 'response_content' in locals() else 'No response'}\")\n",
    "        # Return default values for the batch\n",
    "        return [{\"PRODUCT\": 0, \"PLACE\": 0, \"PRICE\": 0} for _ in tweets]\n",
    "\n",
    "def process_dataframe(df, batch_size=10):\n",
    "    \"\"\"\n",
    "    Process the entire dataframe in batches of specified size\n",
    "    \"\"\"\n",
    "    tweets = df['tweet'].tolist()\n",
    "    all_results = []\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(tweets), batch_size)):\n",
    "        batch = tweets[i:i+batch_size]\n",
    "        batch_results = classify_batch(batch)\n",
    "        all_results.extend(batch_results)\n",
    "        \n",
    "        # Add a small delay to avoid overloading the API\n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Update the dataframe with results\n",
    "    for i, result in enumerate(all_results):\n",
    "        if i < len(df):\n",
    "            df.at[i, 'PRODUCT'] = result.get('PRODUCT', 0)\n",
    "            df.at[i, 'PLACE'] = result.get('PLACE', 0)\n",
    "            df.at[i, 'PRICE'] = result.get('PRICE', 0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Option 2: Process using multithreading for even better performance\n",
    "def classify_tweet_mt(tweet):\n",
    "    \"\"\"Single tweet classification function for multithreading\"\"\"\n",
    "    client = ollama.Client()\n",
    "    prompt = f\"\"\"\n",
    "You are an expert NLP classifier. Given the tweet below, please determine which categories it belongs to among PRODUCT, PLACE, and PRICE.\n",
    "Output the answer in JSON format exactly as:\n",
    "{{\"PRODUCT\": 0 or 1, \"PLACE\": 0 or 1, \"PRICE\": 0 or 1}}\n",
    "\n",
    "Tweet: \"{tweet}\"\n",
    "Only output the JSON.\n",
    "\"\"\"\n",
    "    try:\n",
    "        result_str = client.chat(\n",
    "            model=\"llama3\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            options={\"temperature\": 0}\n",
    "        )\n",
    "        response_content = result_str['message']['content'].strip()\n",
    "        return json.loads(response_content)\n",
    "    except Exception as e:\n",
    "        print(f\"Error classifying tweet: {e}\")\n",
    "        return {\"PRODUCT\": 0, \"PLACE\": 0, \"PRICE\": 0}\n",
    "\n",
    "def process_with_threading(df, max_workers=5):\n",
    "    \"\"\"Process tweets using multithreading\"\"\"\n",
    "    tweets = df['tweet'].tolist()\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(tqdm(executor.map(classify_tweet_mt, tweets), total=len(tweets)))\n",
    "    \n",
    "    # Update the dataframe with results\n",
    "    for i, result in enumerate(results):\n",
    "        df.at[i, 'PRODUCT'] = result.get('PRODUCT', 0)\n",
    "        df.at[i, 'PLACE'] = result.get('PLACE', 0)\n",
    "        df.at[i, 'PRICE'] = result.get('PRICE', 0)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:12<00:00,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                              tweet  PRODUCT  PLACE  \\\n",
      "0   1      I fucking hate starbucks coffee, so expensive        1      0   \n",
      "1   2                 There is no starbucks near my city        0      1   \n",
      "2   3                 I love starbucks caramel malchiato        1      0   \n",
      "3   4  I dont like the new printed notes on the coffe...        1      1   \n",
      "\n",
      "   PRICE  \n",
      "0      1  \n",
      "1      0  \n",
      "2      0  \n",
      "3      0  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Choose your preferred method\n",
    "# Option 1: Use batch processing (recommended for most cases)\n",
    "result_df = process_dataframe(df, batch_size=2)\n",
    "\n",
    "# Option 2: Use multithreading (if your API can handle concurrent requests)\n",
    "# result_df = process_with_threading(df, max_workers=5)\n",
    "\n",
    "print(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
